{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "FIRST FILE : Manage, import and format data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from PIL import ImageOps, Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import glob"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define all datasets paths we might use in the AI training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Dataset(Enum):\n",
    "    TRAIN = \"../datasets/chest_Xray/train/\"\n",
    "    TEST = \"../datasets/chest_Xray/test/\"\n",
    "    VAL = \"../datasets/chest_Xray/val/\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define possible value for each entry we'll have"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Values(Enum):\n",
    "    NORMAL = 0\n",
    "    PNEUMONIA = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import pictures from a chosen dataset, pass it into greyscale if necessary, resize images to a fixed length and transform it to array.\n",
    "This function also fill a 2nd array containing for each image it's value (\"pneumonia\" or \"normal\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def import_dataset(dataset_type, size):\n",
    "    print(\"Loading dataset \" + dataset_type.value)\n",
    "    images = []\n",
    "    for rayX_type in Values:\n",
    "        new_images = glob.glob(dataset_type.value + rayX_type.name + '/*.jpeg')\n",
    "        images += new_images\n",
    "    dataset_x = [[[None]*size]*size]*len(images)\n",
    "    dataset_y = [None]*len(images)\n",
    "    for i in tqdm(range(len(images)), desc=\"Importing images\", colour=\"green\"):\n",
    "        img = Image.open(images[i])\n",
    "        if img.mode != \"L\":\n",
    "            img = ImageOps.grayscale(img)\n",
    "        resized_img = ImageOps.pad(img, (size, size))\n",
    "        arr_img = np.asarray(resized_img)\n",
    "        dataset_x[i] = arr_img\n",
    "        if \"pneumonia\" in images[i].lower():\n",
    "            dataset_y[i] = 1\n",
    "        elif \"normal\" in images[i].lower():\n",
    "            dataset_y[i] = 0\n",
    "\n",
    "    print(\"Returning processed data\")\n",
    "    dataset_x = np.array(dataset_x)\n",
    "    dataset_y = np.array(dataset_y)\n",
    "    return dataset_x, dataset_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Format imported data in 3 steps :\n",
    "    - Reshaping data into 1D array of x² size where x is the border length\n",
    "    - Changing data type from int to float\n",
    "    - Divide each pixel by 255 to fit values between 0 and 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def format_dataset(data_x, size):\n",
    "    print(\"Reshaping data to 1D array\")\n",
    "    data_x = data_x.reshape(len(data_x), size * size)\n",
    "    print(\"Changing type to float\")\n",
    "    data_x = data_x.astype('float32')\n",
    "    print(\"Normalize data\")\n",
    "    data_x /= 255\n",
    "    return data_x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "SECOND FILE : AI file that contains different algorithms and AI types, also contains the saving process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "from keras import Sequential, Model\n",
    "from keras.applications import VGG19\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras import utils as np_utils\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T12:28:32.155069Z",
     "start_time": "2023-05-22T12:28:32.152924Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Variable used to define a global variable representing the size of the image we want, 750 allow us to minimize image size to gain time without losing too much data. If we put more we also sometimes face memory limit errors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "img_length = 750 # If VGG, change to 32 instead of 750"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function looks into saved data if it can find the given AI. If not, it automatically starts the AI training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "def get_ai(ai_name):\n",
    "    if ai_name == \"vgg\":\n",
    "        size = 32\n",
    "    else:\n",
    "        size = img_length\n",
    "    try:\n",
    "        with open(ai_name + \".pkl\", \"rb\") as f:\n",
    "            print(\"Found an existing IA for \" + ai_name)\n",
    "            model = pickle.load(f)\n",
    "            start_with_data(model, size)\n",
    "            return model\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\"Error IA \" + ai_name + \"not found : \", str(ex))\n",
    "        model = start_training(ai_name, size)\n",
    "        if model:\n",
    "            save_ai(model, ai_name)\n",
    "        return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function get a given trained AI and saves it to use the model quicker later"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "def save_ai(ai, ai_name):\n",
    "    try:\n",
    "        with open(ai_name + \".pkl\", \"wb\") as f:\n",
    "            pickle.dump(ai, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"IA model create\")\n",
    "    except Exception as ex:\n",
    "        print(\"Error during pickling object (Possibly unsupported):\", ex)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training begin by loading data that will be used and formatting this data. \"train_x\" and \"test_x\" contains images, \"train_y\" and \"test_y\" contains responses for each image."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "def start_training(ai_name, size):\n",
    "    print(\"Regenerating IA\")\n",
    "    train_x, train_y = dataset.import_dataset(dataset.Dataset.TRAIN, size)\n",
    "    train_x = dataset.format_dataset(train_x, size)\n",
    "    train_y = np_utils.to_categorical(train_y, 2)\n",
    "\n",
    "    test_x, test_y = dataset.import_dataset(dataset.Dataset.TEST, size)\n",
    "    test_x = dataset.format_dataset(test_x, size)\n",
    "    test_y = np_utils.to_categorical(test_y, 2)\n",
    "\n",
    "    # choose your model\n",
    "    match ai_name:\n",
    "        case \"vgg\":\n",
    "            model = vgg_model(train_x, train_y, test_x, test_y, size)\n",
    "        case \"cnn\":\n",
    "            model = cnn_model(train_x, train_y, test_x, test_y, size)\n",
    "        case \"mlp\":\n",
    "            model = mlp_model(train_x, train_y, test_x, test_y)\n",
    "        case _:\n",
    "            model = None\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With the given model this function will test the model on the test data and show scores obtained"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "def start_with_data(model, size):\n",
    "    test_x, test_y = dataset.import_dataset(dataset.Dataset.TEST, size)\n",
    "    test_x = dataset.format_dataset(test_x, size)\n",
    "    test_y = np_utils.to_categorical(test_y, 2)\n",
    "\n",
    "    score(model, test_x, test_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The CNN model (Convolutional Neural Networks) is the first model we experimented, as it seems the most common Image Classifier. It is a Sequential model, we tried multiple setups for the input shape, and the optimizers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "def cnn_model(train_x, train_y, test_x, test_y, size):\n",
    "    # building a linear stack of layers with the sequential model\n",
    "    model = Sequential()\n",
    "    # hidden layer\n",
    "    model.add(Dense(100, input_shape=(size * size,), activation='relu'))\n",
    "    # output layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # looking at the model summary\n",
    "    model.summary()\n",
    "    # compiling the sequential model\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=\"adam\")\n",
    "    # training the model for 10 epochs\n",
    "    history = model.fit(train_x, train_y, batch_size=128, epochs=10, validation_data=(test_x, test_y))\n",
    "\n",
    "    eval_model(history)\n",
    "\n",
    "    score(model, test_x, test_y)\n",
    "\n",
    "    # Calculer la précision du modèle sur l'ensemble de test\n",
    "    test_acc = model.evaluate(test_x, test_y)[1]\n",
    "    print('Précision sur l\\'ensemble de test :', test_acc)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO AVI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "def mlp_model(train_x, train_y, test_x, test_y):\n",
    "    # building a linear stack of layers with the sequential model\n",
    "    model = MLPClassifier(hidden_layer_sizes=(300,), verbose=2)\n",
    "\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    score(model, test_x, test_y)\n",
    "\n",
    "    # Calculer la précision du modèle sur l'ensemble de test\n",
    "    test_acc = model.score(test_x, test_y)\n",
    "    print('Précision sur l\\'ensemble de test :', test_acc)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO SAM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vgg_model(list_x, list_y, test_x, test_y, size):\n",
    "    print('X (vecteur): vgg:  ' + str(list_x.shape))\n",
    "\n",
    "    # Redimonsionner en 32\\\n",
    "    list_x = np.repeat(list_x[..., np.newaxis], 3, axis=-1)\n",
    "    list_x = np.array(list_x).reshape(-1, size, size, 3)\n",
    "    print('X (vecteur) vgg: ' + str(list_x.shape))\n",
    "\n",
    "    test_x = np.repeat(test_x[..., np.newaxis], 3, axis=-1)\n",
    "    test_x = np.array(test_x).reshape(-1, size, size, 3)\n",
    "\n",
    "    print('X (vecteur) vgg: ' + str(test_x.shape))\n",
    "\n",
    "    # Charger le modèle VGG19 pré-entraîné\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(size, size, 3))\n",
    "\n",
    "    # Geler les poids des couches du modèle VGG19\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Ajouter les nouvelles couches de classification\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    # Créer le modèle final en spécifiant les entrées et les sorties\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Compiler le modèle avec une fonction de perte de catégorie croisée, un optimiseur Adam et la métrique de précision\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "    # Entraîner le modèle en utilisant les images d'entraînement et les étiquettes correspondantes\n",
    "    history = model.fit(list_x, list_y, batch_size=128, epochs=10, validation_data=(test_x, test_y))\n",
    "\n",
    "    eval_model(history)\n",
    "    score(model, test_x, test_y)\n",
    "\n",
    "    # Évaluer le modèle sur l'ensemble de test\n",
    "    test_loss, test_acc = model.evaluate(test_x, test_y)\n",
    "    print('Perte sur l\\'ensemble de test :', test_loss)\n",
    "    print('Précision sur l\\'ensemble de test :', test_acc)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The evaluation shows the evolution of the model over epochs and training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "def eval_model(history):\n",
    "    # Validation Accuracy\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Validation Loss\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "    plt.title('Training and validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Score shows the result of the model on the given test data showing for each possible answer how much good answer we have"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def score(model, test_x, test_y):\n",
    "    test_y_pred = np.argmax(model.predict(test_x), axis=-1)\n",
    "    test_y_inverse = np.argmax(test_y, axis=1)\n",
    "\n",
    "    # Calculer la matrice de confusion\n",
    "    cm = confusion_matrix(test_y_inverse, test_y_pred)\n",
    "\n",
    "    # Calculer les pourcentages par classe\n",
    "    cm_percent = cm / cm.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "    unique_labels = np.unique(test_y_inverse)\n",
    "\n",
    "    # Afficher le graphique de matrice de confusion\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, annot_kws={\"fontsize\": 12}, cmap=\"Blues\", cbar=False, fmt='d')\n",
    "    for i in range(len(unique_labels)):\n",
    "        for j in range(len(unique_labels)):\n",
    "            percentage = cm_percent[i, j]\n",
    "            text_color = 'white' if percentage > 0.5 else 'black'\n",
    "            plt.text(j + 0.5, i + 0.5, f\"\\n\\n({percentage:.2%})\", ha='center', va='center', fontsize=12,\n",
    "                     color=text_color)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "\n",
    "    print('L\\'ensemble de test :\\n', test_y_inverse)\n",
    "    print('Prédictions sur l\\'ensemble de test :\\n', test_y_pred)\n",
    "    # print(\"\\n\")\n",
    "    print('20 premieres données de test :\\n', test_y_inverse[:20])\n",
    "    print('20 premieres données de la prediction de test :\\n', test_y_pred[:20])\n",
    "\n",
    "    # # Calculer la précision du modèle sur l'ensemble de test\n",
    "    # test_acc = model.evaluate(test_x, test_y)[1]\n",
    "    # print('Précision sur l\\'ensemble de test :', test_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found an existing IA\n",
      "Loading dataset ../datasets/chest_Xray/test/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing images: 100%|\u001B[32m██████████\u001B[0m| 624/624 [00:05<00:00, 116.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning processed data\n",
      "Reshaping data to 1D array\n",
      "Changing type to float\n",
      "Normalize data\n",
      "L'ensemble de test :\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Prédictions sur l'ensemble de test :\n",
      " [1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1\n",
      " 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
      " 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1\n",
      " 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0\n",
      " 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0\n",
      " 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0\n",
      " 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "20 premieres données de test :\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "20 premieres données de la prediction de test :\n",
      " [1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "ai_cnn = get_ai(\"cnn\")\n",
    "ai_vgg = get_ai(\"vgg\")\n",
    "ai_mlp = get_ai(\"mlp\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
