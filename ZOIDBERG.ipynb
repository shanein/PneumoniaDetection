{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from PIL import ImageOps, Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "class Dataset(Enum):\n",
    "    TRAIN = \"../datasets/chest_Xray/train/\"\n",
    "    TEST = \"../datasets/chest_Xray/test/\"\n",
    "    VAL = \"../datasets/chest_Xray/val/\"\n",
    "\n",
    "\n",
    "class Values(Enum):\n",
    "    NORMAL = 0\n",
    "    PNEUMONIA = 1\n",
    "\n",
    "\n",
    "def import_dataset(dataset_type, img_length):\n",
    "    print(\"Loading dataset \" + dataset_type.value)\n",
    "    images = []\n",
    "    for rayX_type in Values:\n",
    "        new_images = glob.glob(dataset_type.value + rayX_type.name + '/*.jpeg')\n",
    "        images += new_images\n",
    "    dataset_x = [[[None]*img_length]*img_length]*len(images)\n",
    "    dataset_y = [None]*len(images)\n",
    "    for i in tqdm(range(len(images)), desc=\"Importing images\", colour=\"green\"):\n",
    "        img = Image.open(images[i])\n",
    "        if img.mode != \"L\":\n",
    "            img = ImageOps.grayscale(img)\n",
    "        resized_img = ImageOps.pad(img, (img_length, img_length))\n",
    "        arr_img = np.asarray(resized_img)\n",
    "        dataset_x[i] = arr_img\n",
    "        if \"pneumonia\" in images[i].lower():\n",
    "            dataset_y[i] = 1\n",
    "        elif \"normal\" in images[i].lower():\n",
    "            dataset_y[i] = 0\n",
    "\n",
    "    print(\"Returning processed data\")\n",
    "    dataset_x = np.array(dataset_x)\n",
    "    dataset_y = np.array(dataset_y)\n",
    "    return dataset_x, dataset_y\n",
    "\n",
    "\n",
    "def format_dataset(data_x, img_length):\n",
    "    print(\"Reshaping data to 1D array\")\n",
    "    data_x = data_x.reshape(len(data_x), img_length * img_length)\n",
    "    print(\"Changing type to float\")\n",
    "    data_x = data_x.astype('float32')\n",
    "    print(\"Normalize data\")\n",
    "    data_x /= 255\n",
    "    return data_x\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T12:28:32.138261Z",
     "start_time": "2023-05-22T12:28:32.134934Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# import pickle\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from keras import Sequential, Model\n",
    "from keras.applications import VGG19\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras import utils as np_utils\n",
    "from keras.models import save_model, load_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import dataset\n",
    "\n",
    "img_length = 750\n",
    "\n",
    "# vgg model\n",
    "# img_length = 32\n",
    "#\n",
    "\n",
    "#use pickle but error with Adam\n",
    "def get_ai(regenerate=True):\n",
    "    # if regenerate == True:\n",
    "    #     model = start_training()\n",
    "    #     save_ai(model)\n",
    "    #     return model\n",
    "    try:\n",
    "        with open(\"model.pkl\", \"rb\") as f:\n",
    "            print(\"Found an existing IA\")\n",
    "            model = pickle.load(f)\n",
    "            start_with_data(model)\n",
    "            return model\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(\"Error IA not found : \", str(ex))\n",
    "        model = start_training()\n",
    "        save_ai(model)\n",
    "        return model\n",
    "\n",
    "def save_ai(ai):\n",
    "    try:\n",
    "        with open(\"model.pkl\", \"wb\") as f:\n",
    "            pickle.dump(ai, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(\"IA model create\")\n",
    "    except Exception as ex:\n",
    "        print(\"Error during pickling object (Possibly unsupported):\", ex)\n",
    "\n",
    "##\n",
    "\n",
    "#save and load model without pickle\n",
    "# def get_ai(regenerate=True):\n",
    "#     # if regenerate == True:\n",
    "#     #     model = start_training()\n",
    "#     #     save_ai(model)\n",
    "#     #     return model\n",
    "#     try:\n",
    "#         with open(\"model.keras\", \"rb\") as f:\n",
    "#             print(\"Found an existing IA\")\n",
    "#             model = load_model(\"model.keras\")\n",
    "#             start_with_data(model)\n",
    "#             return model\n",
    "#\n",
    "#     except Exception as ex:\n",
    "#         print(\"Error IA not found : \", str(ex))\n",
    "#         model = start_training()\n",
    "#         save_ai(model)\n",
    "#         return model\n",
    "#\n",
    "# def save_ai(ai):\n",
    "#     try:\n",
    "#         ai.save(\"model.keras\")\n",
    "#         print(\"Modèle IA créé et sauvegardé.\")\n",
    "#     except Exception as ex:\n",
    "#         print(\"Erreur lors de la sauvegarde du modèle IA :\", str(ex))\n",
    "\n",
    "##\n",
    "\n",
    "def start_training():\n",
    "    print(\"Regenerating IA\")\n",
    "    train_x, train_y = import_dataset(Dataset.TRAIN, img_length)\n",
    "    train_x = format_dataset(train_x, img_length)\n",
    "    train_y = np_utils.to_categorical(train_y, 2)\n",
    "\n",
    "    test_x, test_y = import_dataset(dataset.Dataset.TEST, img_length)\n",
    "    test_x = format_dataset(test_x, img_length)\n",
    "    test_y = np_utils.to_categorical(test_y, 2)\n",
    "\n",
    "    #choose your model\n",
    "    model = mlp_model(train_x, train_y, test_x, test_y)\n",
    "\n",
    "    return model\n",
    "\n",
    "def start_with_data(model):\n",
    "\n",
    "    test_x, test_y = import_dataset(Dataset.TEST, img_length)\n",
    "    test_x = format_dataset(test_x, img_length)\n",
    "    test_y = np_utils.to_categorical(test_y, 2)\n",
    "\n",
    "    score(model, test_x, test_y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cnn_model(train_x, train_y, test_x, test_y):\n",
    "    # building a linear stack of layers with the sequential model\n",
    "    model = Sequential()\n",
    "    # hidden layer\n",
    "    model.add(Dense(100, input_shape=(img_length * img_length,), activation='relu'))\n",
    "    # output layer\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # looking at the model summary\n",
    "    model.summary()\n",
    "    # compiling the sequential model\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "    # training the model for 10 epochs\n",
    "    model.fit(train_x, train_y, batch_size=128, epochs=10, validation_data=(test_x, test_y))\n",
    "    score(model, test_x, test_y)\n",
    "\n",
    "    # Calculer la précision du modèle sur l'ensemble de test\n",
    "    test_acc = model.evaluate(test_x, test_y)[1]\n",
    "    print('Précision sur l\\'ensemble de test :', test_acc)\n",
    "\n",
    "    return model\n",
    "\n",
    "def mlp_model(train_x, train_y, test_x, test_y):\n",
    "    # building a linear stack of layers with the sequential model\n",
    "    model = MLPClassifier(hidden_layer_sizes=(300,), verbose=2)\n",
    "\n",
    "    model.fit(train_x, train_y)\n",
    "\n",
    "    score(model, test_x, test_y)\n",
    "\n",
    "    # Calculer la précision du modèle sur l'ensemble de test\n",
    "    test_acc = model.score(test_x, test_y)\n",
    "    print('Précision sur l\\'ensemble de test :', test_acc)\n",
    "\n",
    "    return model\n",
    "\n",
    "def vgg_model(list_x, list_y, test_x, test_Y):\n",
    "\n",
    "    print('X (vecteur): vgg:  ' + str(list_x.shape))\n",
    "\n",
    "\n",
    "    #Redimonsionner en 32\\\n",
    "    list_x= np.repeat(list_x[..., np.newaxis], 3, axis=-1)\n",
    "    list_x = np.array(list_x).reshape(-1, img_length, img_length, 3)\n",
    "    print('X (vecteur) vgg: ' + str(list_x.shape))\n",
    "\n",
    "    test_x= np.repeat(test_x[..., np.newaxis], 3, axis=-1)\n",
    "    test_x = np.array(test_x).reshape(-1, img_length, img_length, 3)\n",
    "\n",
    "\n",
    "    print('X (vecteur) vgg: ' + str(test_x.shape))\n",
    "\n",
    "\n",
    "    # Charger le modèle VGG19 pré-entraîné\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_length, img_length, 3))\n",
    "\n",
    "    # Geler les poids des couches du modèle VGG19\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Ajouter les nouvelles couches de classification\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    # Créer le modèle final en spécifiant les entrées et les sorties\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Compiler le modèle avec une fonction de perte de catégorie croisée, un optimiseur Adam et la métrique de précision\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "\n",
    "    # Entraîner le modèle en utilisant les images d'entraînement et les étiquettes correspondantes\n",
    "    model.fit(list_x, list_y, batch_size=128, epochs=10, validation_data=(test_x, test_Y))\n",
    "\n",
    "    score(model, test_x, test_Y)\n",
    "\n",
    "    # Évaluer le modèle sur l'ensemble de test\n",
    "    test_loss, test_acc = model.evaluate(test_x, test_Y)\n",
    "    print('Perte sur l\\'ensemble de test :', test_loss)\n",
    "    print('Précision sur l\\'ensemble de test :', test_acc)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def score(model, test_x, test_y):\n",
    "    test_y_pred = np.argmax(model.predict(test_x), axis=-1)\n",
    "    test_y_inverse = np.argmax(test_y, axis=1)\n",
    "    print('L\\'ensemble de test :\\n', test_y_inverse)\n",
    "    print('Prédictions sur l\\'ensemble de test :\\n', test_y_pred)\n",
    "    # print(\"\\n\")\n",
    "    print('20 premieres données de test :\\n', test_y_inverse[:20])\n",
    "    print('20 premieres données de la prediction de test :\\n', test_y_pred[:20])\n",
    "\n",
    "    # # Calculer la précision du modèle sur l'ensemble de test\n",
    "    # test_acc = model.evaluate(test_x, test_y)[1]\n",
    "    # print('Précision sur l\\'ensemble de test :', test_acc)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T12:28:32.155069Z",
     "start_time": "2023-05-22T12:28:32.152924Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found an existing IA\n",
      "Loading dataset ../datasets/chest_Xray/test/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing images: 100%|\u001B[32m██████████\u001B[0m| 624/624 [00:05<00:00, 116.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning processed data\n",
      "Reshaping data to 1D array\n",
      "Changing type to float\n",
      "Normalize data\n",
      "L'ensemble de test :\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Prédictions sur l'ensemble de test :\n",
      " [1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1\n",
      " 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 0\n",
      " 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 1\n",
      " 1 0 0 0 0 1 1 0 0 1 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 1 0\n",
      " 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 0 1 0\n",
      " 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 0 0 1 0\n",
      " 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "20 premieres données de test :\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "20 premieres données de la prediction de test :\n",
      " [1 0 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "ai = get_ai(True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-22T12:28:40.384671Z",
     "start_time": "2023-05-22T12:28:32.156067Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
